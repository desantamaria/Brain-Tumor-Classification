{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dksUvWDM5wkl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Uw4ADj6-Oj",
        "outputId": "5674d969-44f4-4ac5-e30f-2a3d1c19e3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading brain-tumor-mri-dataset.zip to /content\n",
            " 96% 142M/149M [00:00<00:00, 183MB/s]\n",
            "100% 149M/149M [00:00<00:00, 181MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Am-Lym2V7hmR"
      },
      "outputs": [],
      "source": [
        "def get_class_paths(path):\n",
        "  classes = []\n",
        "  class_paths = []\n",
        "\n",
        "  # Iterate through directories in the training path\n",
        "  for label in os.listdir(path):\n",
        "    label_path = os.path.join(path, label)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(label_path):\n",
        "      # Iterate through images in the label directory\n",
        "      for image in os.listdir(label_path):\n",
        "        image_path = os.path.join(label_path, image)\n",
        "\n",
        "        # Add class and path to respective lists\n",
        "        classes.append(label)\n",
        "        class_paths.append(image_path)\n",
        "\n",
        "  # Create a DataFrame with the collected data\n",
        "  df = pd.DataFrame({\n",
        "      'Class Path': class_paths,\n",
        "      'Class': classes\n",
        "  })\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gEhFWVSQ9MDF"
      },
      "outputs": [],
      "source": [
        "tr_df = get_class_paths(\"/content/Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "XRPAd2jU9Za6"
      },
      "outputs": [],
      "source": [
        "ts_df = get_class_paths(\"/content/Testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CPffG03A_ly2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import regularizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "PGSJcwCUAigK"
      },
      "outputs": [],
      "source": [
        "valid_df, ts_df = train_test_split(ts_df, train_size=0.5, stratify=ts_df['Class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sNRETE4DBSFU"
      },
      "outputs": [],
      "source": [
        "# Standardize Image Size and Brightness to improve consistency.\n",
        "batch_size = 32\n",
        "\n",
        "img_size = (299, 299)\n",
        "\n",
        "image_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n",
        "\n",
        "ts_gen = ImageDataGenerator(rescale=1/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWdbhY6Mrn62",
        "outputId": "fdc47ba9-fc0a-43c6-ac42-0eae5a1879fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5712 validated image filenames belonging to 4 classes.\n",
            "Found 656 validated image filenames belonging to 4 classes.\n",
            "Found 655 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "# Standardize Image Size and Brightness to improve consistency.\n",
        "batch_size = 16\n",
        "\n",
        "img_size = (224, 224)\n",
        "\n",
        "image_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n",
        "\n",
        "ts_gen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "\n",
        "# Takes Traning DF and creates flow of images from DF\n",
        "tr_gen = image_generator.flow_from_dataframe(\n",
        "    tr_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=batch_size,\n",
        "    target_size=img_size\n",
        ")\n",
        "\n",
        "# Keeping order of Testing Data consistent\n",
        "ts_gen = ts_gen.flow_from_dataframe(\n",
        "    ts_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=16,\n",
        "    target_size=img_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "valid_gen = image_generator.flow_from_dataframe(\n",
        "    valid_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=batch_size,\n",
        "    target_size=img_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOrgeQU4AlSX",
        "outputId": "1a1d3d6d-8c80-40ac-9048-5060c65c2376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# First Convolutional Block\n",
        "cnn_model.add(Conv2D(256, (3, 3), padding='same', input_shape=(224, 224, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolutional Block\n",
        "cnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn_model.add(Dropout(0.20))\n",
        "\n",
        "# Third Convolutional Block\n",
        "cnn_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "# Dense layers\n",
        "cnn_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_model.add(Dropout(0.35))\n",
        "\n",
        "# Output layer\n",
        "cnn_model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(Adamax(learning_rate = 0.001), loss='categorical_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVUYAk4r6ali",
        "outputId": "a607b2ff-84d8-49fd-d8ac-ac9a07bc104d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m 87/357\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.5143 - loss: 13.3994 - precision_2: 0.5268 - recall_2: 0.4924"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:17:55+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-28ed7da6-d948-478d-adfd-c7a447b3f847 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:17:55+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-28ed7da6-d948-478d-adfd-c7a447b3f847 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m142/357\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 125ms/step - accuracy: 0.5343 - loss: 11.5170 - precision_2: 0.5618 - recall_2: 0.4884"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = cnn_model.fit(\n",
        "    tr_gen,\n",
        "    epochs=11,\n",
        "    validation_data=valid_gen,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chkx31A3Ki1B"
      },
      "outputs": [],
      "source": [
        "# Getting training and validation metrics from history\n",
        "metrics = ['accuracy', 'loss', 'precision', 'recall']\n",
        "tr_metrics = {m: history.history[m] for m in metrics}\n",
        "val_metrics = {m: history.history[f'val_{m}'] for m in metrics}\n",
        "\n",
        "# Find best epochs and values\n",
        "best_epochs = {}\n",
        "best_values = {}\n",
        "for m in metrics:\n",
        "  if m == 'loss':\n",
        "    idx = np.argmin(val_metrics[m])\n",
        "  else:\n",
        "    idx = np.argmax(val_metrics[m])\n",
        "  best_epochs[m] = idx + 1\n",
        "  best_values[m] = val_metrics[m][idx]\n",
        "\n",
        "# Plot metrics\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "for i, metric in enumerate(metrics, 1):\n",
        "  plt.subplot(2, 2, i)\n",
        "  epochs = range(1, len(tr_metrics[metric]) + 1)\n",
        "\n",
        "  plt.plot(epochs, tr_metrics[metric], 'r', label=f'Training {metric}')\n",
        "  plt.plot(epochs, val_metrics[metric], 'g', label=f'Validation {metric}')\n",
        "  plt.scatter(best_epochs[metric], best_values[metric], s=150, c='blue',\n",
        "              label=f'Best epoch = {best_epochs[metric]}')\n",
        "  plt.title(f'Training and Validation {metric.title()}')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(metric.title())\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plt.suptitle('Model Training Metrics Over Epochs', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJqe_hmgKwec"
      },
      "outputs": [],
      "source": [
        "train_score =  cnn_model.evaluate(tr_gen, verbose=1)\n",
        "valid_score = cnn_model.evaluate(valid_gen, verbose=1)\n",
        "test_score = cnn_model.evaluate(ts_gen, verbose=1)\n",
        "\n",
        "print(f'Traning Accuracy: {train_score[1]*100:.2f}%')\n",
        "print(f'Train Loss: {train_score[0]:.4f}')\n",
        "print(f'\\n\\nValidation Accuracy: {valid_score[1]*100:.2f}%')\n",
        "print(f'Validation Loss: {valid_score[0]:.4f}')\n",
        "print(f'\\n\\nTest Accuracy: {test_score[1]*100:.2f}%')\n",
        "print(f'Test Loss: {test_score[0]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMSCm1AlLEFe"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix to see where models succeeds and fails\n",
        "preds = cnn_model.predict(ts_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "class_dict = {\n",
        "    0: 'glioma',\n",
        "    1: 'meningioma',\n",
        "    2: 'no_tumor',\n",
        "    3: 'pituitary'\n",
        "}\n",
        "\n",
        "# Then create and display the confusion matrix\n",
        "cm = confusion_matrix(ts_gen.classes, y_pred)\n",
        "labels = list(class_dict.keys())\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkAKqLz7LUJH"
      },
      "outputs": [],
      "source": [
        "clr = classification_report(ts_gen.classes, y_pred)\n",
        "print(clr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB04rh4NMHj8"
      },
      "outputs": [],
      "source": [
        "cnn_model.save(\"cnn_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMXOzG+UYcV1RdYNMqe3U5a",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
