{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXOzG+UYcV1RdYNMqe3U5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desantamaria/Brain-Tumor-Classification/blob/main/Brain_Tumor_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dksUvWDM5wkl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Uw4ADj6-Oj",
        "outputId": "5674d969-44f4-4ac5-e30f-2a3d1c19e3c4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading brain-tumor-mri-dataset.zip to /content\n",
            " 96% 142M/149M [00:00<00:00, 183MB/s]\n",
            "100% 149M/149M [00:00<00:00, 181MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_paths(path):\n",
        "  classes = []\n",
        "  class_paths = []\n",
        "\n",
        "  # Iterate through directories in the training path\n",
        "  for label in os.listdir(path):\n",
        "    label_path = os.path.join(path, label)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(label_path):\n",
        "      # Iterate through images in the label directory\n",
        "      for image in os.listdir(label_path):\n",
        "        image_path = os.path.join(label_path, image)\n",
        "\n",
        "        # Add class and path to respective lists\n",
        "        classes.append(label)\n",
        "        class_paths.append(image_path)\n",
        "\n",
        "  # Create a DataFrame with the collected data\n",
        "  df = pd.DataFrame({\n",
        "      'Class Path': class_paths,\n",
        "      'Class': classes\n",
        "  })\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Am-Lym2V7hmR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_df = get_class_paths(\"/content/Training\")"
      ],
      "metadata": {
        "id": "gEhFWVSQ9MDF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df = get_class_paths(\"/content/Testing\")"
      ],
      "metadata": {
        "id": "XRPAd2jU9Za6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import regularizers\n"
      ],
      "metadata": {
        "id": "CPffG03A_ly2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df, ts_df = train_test_split(ts_df, train_size=0.5, stratify=ts_df['Class'])"
      ],
      "metadata": {
        "id": "PGSJcwCUAigK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Image Size and Brightness to improve consistency.\n",
        "batch_size = 32\n",
        "\n",
        "img_size = (299, 299)\n",
        "\n",
        "image_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n",
        "\n",
        "ts_gen = ImageDataGenerator(rescale=1/255)"
      ],
      "metadata": {
        "id": "sNRETE4DBSFU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Image Size and Brightness to improve consistency.\n",
        "batch_size = 16\n",
        "\n",
        "img_size = (224, 224)\n",
        "\n",
        "image_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n",
        "\n",
        "ts_gen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "\n",
        "# Takes Traning DF and creates flow of images from DF\n",
        "tr_gen = image_generator.flow_from_dataframe(\n",
        "    tr_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=batch_size,\n",
        "    target_size=img_size\n",
        ")\n",
        "\n",
        "# Keeping order of Testing Data consistent\n",
        "ts_gen = ts_gen.flow_from_dataframe(\n",
        "    ts_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=16,\n",
        "    target_size=img_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "valid_gen = image_generator.flow_from_dataframe(\n",
        "    valid_df,\n",
        "    x_col='Class Path',\n",
        "    y_col='Class',\n",
        "    batch_size=batch_size,\n",
        "    target_size=img_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWdbhY6Mrn62",
        "outputId": "fdc47ba9-fc0a-43c6-ac42-0eae5a1879fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5712 validated image filenames belonging to 4 classes.\n",
            "Found 656 validated image filenames belonging to 4 classes.\n",
            "Found 655 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# First Convolutional Block\n",
        "cnn_model.add(Conv2D(256, (3, 3), padding='same', input_shape=(224, 224, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolutional Block\n",
        "cnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn_model.add(Dropout(0.20))\n",
        "\n",
        "# Third Convolutional Block\n",
        "cnn_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "# Dense layers\n",
        "cnn_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_model.add(Dropout(0.35))\n",
        "\n",
        "# Output layer\n",
        "cnn_model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(Adamax(learning_rate = 0.001), loss='categorical_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOrgeQU4AlSX",
        "outputId": "1a1d3d6d-8c80-40ac-9048-5060c65c2376"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = cnn_model.fit(\n",
        "    tr_gen,\n",
        "    epochs=11,\n",
        "    validation_data=valid_gen,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVUYAk4r6ali",
        "outputId": "a607b2ff-84d8-49fd-d8ac-ac9a07bc104d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 87/357\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.5143 - loss: 13.3994 - precision_2: 0.5268 - recall_2: 0.4924"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:17:55+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-28ed7da6-d948-478d-adfd-c7a447b3f847 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:17:55+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-28ed7da6-d948-478d-adfd-c7a447b3f847 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m142/357\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 125ms/step - accuracy: 0.5343 - loss: 11.5170 - precision_2: 0.5618 - recall_2: 0.4884"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting training and validation metrics from history\n",
        "metrics = ['accuracy', 'loss', 'precision', 'recall']\n",
        "tr_metrics = {m: history.history[m] for m in metrics}\n",
        "val_metrics = {m: history.history[f'val_{m}'] for m in metrics}\n",
        "\n",
        "# Find best epochs and values\n",
        "best_epochs = {}\n",
        "best_values = {}\n",
        "for m in metrics:\n",
        "  if m == 'loss':\n",
        "    idx = np.argmin(val_metrics[m])\n",
        "  else:\n",
        "    idx = np.argmax(val_metrics[m])\n",
        "  best_epochs[m] = idx + 1\n",
        "  best_values[m] = val_metrics[m][idx]\n",
        "\n",
        "# Plot metrics\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "for i, metric in enumerate(metrics, 1):\n",
        "  plt.subplot(2, 2, i)\n",
        "  epochs = range(1, len(tr_metrics[metric]) + 1)\n",
        "\n",
        "  plt.plot(epochs, tr_metrics[metric], 'r', label=f'Training {metric}')\n",
        "  plt.plot(epochs, val_metrics[metric], 'g', label=f'Validation {metric}')\n",
        "  plt.scatter(best_epochs[metric], best_values[metric], s=150, c='blue',\n",
        "              label=f'Best epoch = {best_epochs[metric]}')\n",
        "  plt.title(f'Training and Validation {metric.title()}')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(metric.title())\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plt.suptitle('Model Training Metrics Over Epochs', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Chkx31A3Ki1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_score =  cnn_model.evaluate(tr_gen, verbose=1)\n",
        "valid_score = cnn_model.evaluate(valid_gen, verbose=1)\n",
        "test_score = cnn_model.evaluate(ts_gen, verbose=1)\n",
        "\n",
        "print(f'Traning Accuracy: {train_score[1]*100:.2f}%')\n",
        "print(f'Train Loss: {train_score[0]:.4f}')\n",
        "print(f'\\n\\nValidation Accuracy: {valid_score[1]*100:.2f}%')\n",
        "print(f'Validation Loss: {valid_score[0]:.4f}')\n",
        "print(f'\\n\\nTest Accuracy: {test_score[1]*100:.2f}%')\n",
        "print(f'Test Loss: {test_score[0]:.4f}')"
      ],
      "metadata": {
        "id": "YJqe_hmgKwec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix to see where models succeeds and fails\n",
        "preds = cnn_model.predict(ts_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "class_dict = {\n",
        "    0: 'glioma',\n",
        "    1: 'meningioma',\n",
        "    2: 'no_tumor',\n",
        "    3: 'pituitary'\n",
        "}\n",
        "\n",
        "# Then create and display the confusion matrix\n",
        "cm = confusion_matrix(ts_gen.classes, y_pred)\n",
        "labels = list(class_dict.keys())\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TMSCm1AlLEFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clr = classification_report(ts_gen.classes, y_pred)\n",
        "print(clr)"
      ],
      "metadata": {
        "id": "PkAKqLz7LUJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save(\"cnn_model.h5\")"
      ],
      "metadata": {
        "id": "SB04rh4NMHj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Streamlit Web App**"
      ],
      "metadata": {
        "id": "mK_4WEfgbbdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Sequential model\n",
        "\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "cnn_model.add(Conv2D(256, (3, 3), padding='same', input_shape=(224, 224, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn_model.add(Dropout(0.20))\n",
        "\n",
        "# Flatten the output for fully connected layers\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "cnn_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "cnn_model.add(Dropout(0.35))\n",
        "\n",
        "cnn_model.add(Dense(4, activation='softmax')) # Output layer with 4 neurons for the 4 classes\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(Adamax(learning_rate = 0.001), loss='categorical_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Display the model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "daq9EtE70iMT",
        "outputId": "fab3c108-0550-45bf-a7e1-5713c56e4bbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Sequential' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4434abe7e3f6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a Sequential model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit pyngrok python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyupeUBpabRr",
        "outputId": "26941d4f-e473-466f-e29a-86549b0edb10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.40.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.79.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting appdirs>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai)\n",
            "  Downloading auth0_python-4.7.2-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting chromadb>=0.4.24 (from crewai)\n",
            "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai) (8.1.7)\n",
            "Collecting crewai-tools>=0.14.0 (from crewai)\n",
            "  Downloading crewai_tools-0.14.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting instructor>=1.3.3 (from crewai)\n",
            "  Downloading instructor-1.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.30.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from crewai) (0.3.7)\n",
            "Collecting litellm>=1.44.22 (from crewai)\n",
            "  Downloading litellm-1.52.5-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.54.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.28.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.28.0)\n",
            "Requirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.9.2)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.0.1)\n",
            "Collecting pyvis>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai) (2024.9.11)\n",
            "Collecting tomli-w>=1.1.0 (from crewai)\n",
            "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: tomli>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.0.2)\n",
            "Collecting uv>=0.4.25 (from crewai)\n",
            "  Downloading uv-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.15)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (3.10.10)\n",
            "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (2.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (2.2.3)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (1.67.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.4.24->crewai)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai) (13.9.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from crewai-tools>=0.14.0->crewai) (4.12.3)\n",
            "Collecting docker>=7.1.0 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting docx2txt>=0.8 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting embedchain>=0.1.114 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading embedchain-0.1.125-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting lancedb>=0.5.4 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting pyright>=1.1.350 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pyright-1.1.388-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytest>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai-tools>=0.14.0->crewai) (8.3.3)\n",
            "Collecting pytube>=15.0.0 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting selenium>=4.18.1 (from crewai-tools>=0.14.0->crewai)\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (3.1.4)\n",
            "Collecting jiter<0.6.0,>=0.5.0 (from instructor>=1.3.3->crewai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (2.23.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai) (2.0.36)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai) (0.1.142)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (24.2)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai) (8.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai) (4.23.0)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.44.22->crewai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.22.0->crewai) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.1->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-api>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-sdk>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai) (3.4.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai) (3.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.12.3->crewai-tools>=0.14.0->crewai) (2.6)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai) (1.16.0)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading cohere-5.11.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.71.1)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading langchain_cohere-0.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting mem0ai<0.2.0,>=0.1.29 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading mem0ai-0.1.29-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypdf<6.0.0,>=5.0.0 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.44.22->crewai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb>=0.4.24->crewai)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.24->crewai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.24->crewai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.24->crewai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai) (3.20.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.3.3->crewai) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.21.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting deprecation (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pylance==0.19.1 (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai) (5.5.0)\n",
            "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.19.1->lancedb>=0.5.4->crewai-tools>=0.14.0->crewai) (17.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.2.16->crewai) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai)\n",
            "  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.4.24->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.4.24->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai-tools>=0.14.0->crewai) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai-tools>=0.14.0->crewai) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->auth0-python>=4.7.1->crewai) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb>=0.4.24->crewai) (3.0.0)\n",
            "Collecting trio~=0.17 (from selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.16->crewai) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb>=0.4.24->crewai) (0.26.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb>=0.4.24->crewai) (1.5.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai)\n",
            "  Downloading websockets-14.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (4.9)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.19.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.25.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.13.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai) (2024.10.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Collecting langchain-experimental>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (0.9.0)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain>=0.2.16->crewai)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain_groq)\n",
            "  Downloading langchain_core-0.3.17-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.24->crewai) (0.1.2)\n",
            "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2024.2)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading qdrant_client-1.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai-tools>=0.14.0->crewai) (1.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (0.2.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai) (1.3.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.62.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (0.13.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2024.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.24->crewai) (0.6.1)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading grpcio_tools-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.6.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading crewai-0.79.4-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.7.2-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.18-py3-none-any.whl (615 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai_tools-0.14.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.0/463.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.6.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.30.1-py3-none-any.whl (18 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading litellm-1.52.5-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.28.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.28.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading uv-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.125-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl (27.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.1/27.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl (30.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyright-1.1.388-py3-none-any.whl (18 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cohere-5.11.3-py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.7/248.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.17-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mem0ai-0.1.29-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading qdrant_client-1.12.1-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading grpcio_tools-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: docx2txt, pypika\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=71b3e0e514d8fbea0e9a7d731f466f35c50f04166c1ed77f1e0c2b8dc5c2a7d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=37a1e9be8194220f091a4e35575354a686c6c3174360d0ee7e5b7049c4ffa5e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built docx2txt pypika\n",
            "Installing collected packages: sortedcontainers, schema, pypika, monotonic, durationpy, docx2txt, appdirs, wsproto, websockets, uvloop, uvicorn, uv, types-requests, tomli-w, SQLAlchemy, pytube, pysbd, pyproject_hooks, pypdf, protobuf, portalocker, parameterized, overrides, outcome, opentelemetry-util-http, nodeenv, mypy-extensions, mmh3, marshmallow, Mako, jsonref, json-repair, jiter, jedi, hyperframe, humanfriendly, httpx-sse, httptools, hpack, fastavro, deprecation, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, trio, tiktoken, starlette, pyright, pylance, posthog, opentelemetry-proto, opentelemetry-api, h2, grpcio-tools, gptcache, docker, coloredlogs, build, alembic, trio-websocket, pyvis, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, lancedb, kubernetes, groq, fastapi, dataclasses-json, selenium, qdrant-client, opentelemetry-sdk, opentelemetry-instrumentation, litellm, langchain-core, instructor, cohere, auth0-python, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, langchain-openai, langchain_groq, opentelemetry-instrumentation-fastapi, langchain-community, chromadb, langchain-experimental, langchain-cohere, embedchain, crewai-tools, crewai\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.0\n",
            "    Uninstalling jiter-0.7.0:\n",
            "      Successfully uninstalled jiter-0.7.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.0\n",
            "    Uninstalling opentelemetry-api-1.28.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.0\n",
            "    Uninstalling opentelemetry-sdk-1.28.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.15\n",
            "    Uninstalling langchain-core-0.3.15:\n",
            "      Successfully uninstalled langchain-core-0.3.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 SQLAlchemy-2.0.35 alembic-1.14.0 appdirs-1.4.4 asgiref-3.8.1 auth0-python-4.7.2 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.18 cohere-5.11.3 coloredlogs-15.0.1 crewai-0.79.4 crewai-tools-0.14.0 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 docx2txt-0.8 durationpy-0.9 embedchain-0.1.125 fastapi-0.115.5 fastavro-1.9.7 gptcache-0.1.44 groq-0.12.0 grpcio-tools-1.67.1 h2-4.1.0 hpack-4.0.0 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 hyperframe-6.0.1 instructor-1.6.3 jedi-0.19.2 jiter-0.5.0 json-repair-0.30.1 jsonref-1.1.0 kubernetes-31.0.0 lancedb-0.15.0 langchain-cohere-0.3.1 langchain-community-0.3.7 langchain-core-0.3.17 langchain-experimental-0.3.3 langchain-openai-0.2.8 langchain_groq-0.2.1 litellm-1.52.5 marshmallow-3.23.1 mem0ai-0.1.29 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.20.0 opentelemetry-api-1.28.1 opentelemetry-exporter-otlp-proto-common-1.28.1 opentelemetry-exporter-otlp-proto-grpc-1.28.1 opentelemetry-exporter-otlp-proto-http-1.28.1 opentelemetry-instrumentation-0.49b1 opentelemetry-instrumentation-asgi-0.49b1 opentelemetry-instrumentation-fastapi-0.49b1 opentelemetry-proto-1.28.1 opentelemetry-sdk-1.28.1 opentelemetry-semantic-conventions-0.49b1 opentelemetry-util-http-0.49b1 outcome-1.3.0.post0 overrides-7.7.0 parameterized-0.9.0 portalocker-2.10.1 posthog-3.7.0 protobuf-5.28.3 pydantic-settings-2.6.1 pylance-0.19.1 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.388 pysbd-0.3.4 pytube-15.0.0 pyvis-0.3.2 qdrant-client-1.12.1 schema-0.7.7 selenium-4.26.1 sortedcontainers-2.4.0 starlette-0.41.2 tiktoken-0.7.0 tomli-w-1.1.0 trio-0.27.0 trio-websocket-0.11.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uv-0.5.1 uvicorn-0.32.0 uvloop-0.21.0 watchfiles-0.24.0 websockets-14.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "!pkill ngrok\n"
      ],
      "metadata": {
        "id": "DemTYhZxbqSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46d4823-554c-439d-deff-7176d4ffc484"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:22:19+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-a6cc88f8-e091-4584-b974-a16f6adb5e01 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:22:19+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-a6cc88f8-e091-4584-b974-a16f6adb5e01 err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)"
      ],
      "metadata": {
        "id": "U4_d_tftbv1S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit():\n",
        "  os.system('streamlit run /content/app.py --server.port 8501')"
      ],
      "metadata": {
        "id": "PIZVHqRjdCQb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import PIL.Image\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key='')\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "output_dir = 'saliency_maps'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def generate_explanation(img_path, model_prediction, confidence, gen_model):\n",
        "\n",
        "  prompt = f\"\"\" MRI Analysis System Prompt\n",
        "  You are an expert neuroradiologist with extensive experience in tumor diagnosis and medical imaging interpretation. Your task is to analyze and explain the relationship between an MRI scan's saliency map and its automated classification results.\n",
        "  Context Parameters\n",
        "\n",
        "  Primary Image: Brain MRI scan\n",
        "  Analysis Type: Saliency map interpretation\n",
        "  Model Prediction: {model_prediction}\n",
        "  Confidence Score: {confidence * 100}%\n",
        "  Possible Classifications: Glioma, Meningioma, Pituitary, No Tumor\n",
        "\n",
        "  Required Analysis Components\n",
        "  1. Anatomical Analysis\n",
        "\n",
        "  Identify specific brain structures and regions highlighted in the saliency map\n",
        "  Reference standard anatomical landmarks\n",
        "  Note spatial relationships to critical brain structures\n",
        "  Describe tumor location using standardized anatomical terminology\n",
        "\n",
        "  2. Classification Justification\n",
        "\n",
        "  Correlate highlighted regions with typical presentation patterns for the predicted tumor type\n",
        "  Compare findings against known radiological features of the predicted class\n",
        "  Explain why these features support the model's classification\n",
        "  Address any atypical presentations if present\n",
        "\n",
        "  3. Clinical Context\n",
        "\n",
        "  Discuss the typical characteristics of the predicted tumor type\n",
        "  Note common symptoms associated with tumors in the highlighted locations\n",
        "  Reference relevant prognostic implications\n",
        "  Suggest appropriate follow-up imaging or diagnostic procedures\n",
        "\n",
        "  4. Technical Assessment\n",
        "\n",
        "  Evaluate the strength of the model's prediction based on:\n",
        "\n",
        "  Confidence score\n",
        "  Anatomical accuracy of highlighted regions\n",
        "  Consistency with clinical presentation patterns\n",
        "\n",
        "\n",
        "  Note any technical limitations or artifacts\n",
        "\n",
        "  <Output Format>\n",
        "\n",
        "  **Radiological Analysis Report**\n",
        "\n",
        "  **Anatomical Findings:**\n",
        "  [2-3 sentences describing the specific brain regions highlighted and their anatomical significance]\n",
        "\n",
        "  **Classification Analysis:**\n",
        "  [2-3 sentences explaining how the highlighted regions support the model's classification]\n",
        "\n",
        "  **Clinical Implications:**\n",
        "  [2-3 sentences discussing the medical significance and recommended next steps]\n",
        "\n",
        "  **Technical Assessment:**\n",
        "  [1-2 sentences evaluating the reliability of the model's prediction]\n",
        "\n",
        "  **Key Features Supporting Classification:**\n",
        "  - [Bullet point 1]\n",
        "  - [Bullet point 2]\n",
        "  - [Bullet point 3]\n",
        "\n",
        "  **Recommended Follow-up:**\n",
        "  [1-2 specific recommendations for clinical validation or further testing]\n",
        "  </Output Format>\n",
        "\n",
        "  Response Guidelines\n",
        "\n",
        "  Maintain professional medical terminology while ensuring clarity\n",
        "  Support observations with anatomical reasoning\n",
        "  Be specific about location and extent of highlighted regions\n",
        "  Connect anatomical findings to clinical significance\n",
        "  Maintain appropriate clinical uncertainty where warranted\n",
        "  Avoid technical discussion of the AI model's architecture or general saliency map properties\n",
        "  Focus on medical interpretation rather than technical aspects\n",
        "  Include clear, actionable next steps for clinical validation\n",
        "\n",
        "  Constraints\n",
        "\n",
        "  Keep each section concise and focused\n",
        "  Use standard medical terminology with brief clarifications where needed\n",
        "  Avoid speculation about patient-specific outcomes\n",
        "  Maintain focus on radiological findings rather than treatment planning\n",
        "  Acknowledge limitations of AI-based analysis when appropriate\n",
        "\n",
        "  Please verify step by step.\n",
        "  \"\"\"\n",
        "\n",
        "  img = PIL.Image.open(img_path)\n",
        "\n",
        "  if(gen_model == \"Gemini\"):\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "    response = model.generate_content([prompt, img])\n",
        "    return response.text\n",
        "  else:\n",
        "    raw_response = client.chat.completions.create(model=\"llama-3.2-3b-preview\",\n",
        "                                                    messages=[{\n",
        "                                                        \"role\": \"user\",\n",
        "                                                        \"content\": prompt\n",
        "                                                    }])\n",
        "    return raw_response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "def generate_saliency_map(model, img_array, class_index, img_size):\n",
        "  with tf.GradientTape() as tape:\n",
        "    img_tensor = tf.convert_to_tensor(img_array)\n",
        "    tape.watch(img_tensor)\n",
        "    predictions = model(img_tensor)\n",
        "    target_class = predictions[:, class_index]\n",
        "\n",
        "  gradients = tape.gradient(target_class, img_tensor)\n",
        "  gradients = tf.math.abs(gradients)\n",
        "  gradients = tf.reduce_max(gradients, axis=-1)\n",
        "  gradients = gradients.numpy().squeeze()\n",
        "\n",
        "  # Resize gradients to match original image size\n",
        "  gradients = cv2.resize(gradients, img_size)\n",
        "\n",
        "  # Create a circular mask for the brain area\n",
        "  center = (gradients.shape[0] // 2, gradients.shape[1] // 2)\n",
        "  radius = min(center[0], center[1]) - 10\n",
        "  y, x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]\n",
        "  mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2\n",
        "\n",
        "  # Apply mask to gradients\n",
        "  gradients = gradients * mask\n",
        "\n",
        "  # Normalize only the brain area\n",
        "  brain_gradients = gradients[mask]\n",
        "  if brain_gradients.max() > brain_gradients.min():\n",
        "    brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max() - brain_gradients.min())\n",
        "  gradients[mask] = brain_gradients\n",
        "\n",
        "  # Apply a higher threshold\n",
        "  threshold = np.percentile(gradients[mask], 80)\n",
        "  gradients[gradients < threshold] = 0\n",
        "\n",
        "  # Apply more aggressive smoothing\n",
        "  gradients = cv2.GaussianBlur(gradients, (11, 11), 0)\n",
        "\n",
        "  # Create a heatmap overlay with enhanced contrast\n",
        "  heatmap = cv2.applyColorMap(np.uint8(255 * gradients), cv2.COLORMAP_JET)\n",
        "  heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Resize heatmap to match original image size\n",
        "  heatmap = cv2.resize(heatmap, img_size)\n",
        "\n",
        "  # Superimpose the heatmap on original image with increased opacity\n",
        "  original_img = image.img_to_array(img)\n",
        "  superimposed_img = heatmap * 0.7 + original_img * 0.3\n",
        "  superimposed_img = superimposed_img.astype(np.uint8)\n",
        "\n",
        "  img_path = os.path.join(output_dir, uploaded_file.name)\n",
        "  with open(img_path, \"wb\") as f:\n",
        "    f.write(uploaded_file.getbuffer())\n",
        "\n",
        "  saliency_map_path = f\"saliency_maps/{uploaded_file.name}\"\n",
        "\n",
        "  # Save the saliency map\n",
        "  cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "  return superimposed_img\n",
        "\n",
        "def load_xception_model(model_path):\n",
        "  img_shape=(299, 299, 3)\n",
        "  base_model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\",\n",
        "                                              input_shape=img_shape, pooling='max')\n",
        "\n",
        "  model = Sequential([\n",
        "      base_model,\n",
        "      Flatten(),\n",
        "      Dropout(rate=0.3),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dropout(rate=0.25),\n",
        "      Dense(4, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.build((None,) + img_shape)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(Adamax(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy',\n",
        "                         Precision(),\n",
        "                         Recall()])\n",
        "\n",
        "  model.load_weights(model_path)\n",
        "\n",
        "  return model\n",
        "\n",
        "st.title(\"Brain Tumor Classification\")\n",
        "\n",
        "st.write(\"Upload an image of a brain MRI scan to classify.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "\n",
        "  selected_model = st.radio(\n",
        "    \"Select Model\",\n",
        "    (\"Transfer Learning - Xception\", \"Custom Convolutional Neural Network\")\n",
        "  )\n",
        "\n",
        "  if selected_model == \"Transfer Learning - Xception\":\n",
        "    model = load_xception_model('/content/xception_model.weights.h5')\n",
        "    img_size = (299, 299)\n",
        "  else:\n",
        "    model = load_model('/content/cnn_model.h5')\n",
        "    img_size = (224, 224)\n",
        "\n",
        "  labels = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "  img = image.load_img(uploaded_file, target_size=img_size)\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0)\n",
        "  img_array /= 255.0\n",
        "\n",
        "\n",
        "  with st.spinner('Generating Prediction...'):\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "  # Get the class with the highest probability\n",
        "  class_index = np.argmax(prediction[0])\n",
        "  result = labels[class_index]\n",
        "\n",
        "  st.write(f\"Prediction: {result}\")\n",
        "  st.write(\"Prediction:\")\n",
        "  for label, prob in zip(labels, prediction[0]):\n",
        "    st.write(f\"{label}: {prob:.4f}\")\n",
        "\n",
        "  with st.spinner('Generating saliency map...'):\n",
        "    saliency_map = generate_saliency_map(model, img_array, class_index, img_size)\n",
        "\n",
        "  col1, col2 = st.columns(2)\n",
        "  with col1:\n",
        "    st.image(uploaded_file, caption=\"Uploaded Image\", use_container_width=True)\n",
        "  with col2:\n",
        "    st.image(saliency_map, caption=\"Saliency Map\", use_container_width=True)\n",
        "\n",
        "  st.write('## Classification Results')\n",
        "\n",
        "  result_container = st.container()\n",
        "  result_container.markdown(\n",
        "      f\"\"\"\n",
        "      <div style=\"background-color: #000000; color: #ffffff; paddingL 30px; border-radius: 15px;\">\n",
        "        <div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
        "          <div style=\"flex: 1; text-align: center;\">\n",
        "            <h3 style=\"color: #ffffff; margin-bottom: 10px; font-size: 20px;\">Prediction</h3>\n",
        "            <p style=\"font-size: 36px; font-weight: 800; color: #FF0000; margin: 0;\">\n",
        "              {result}\n",
        "            </p>\n",
        "          </div>\n",
        "          <div style=\"width: 2px; height: 80px; background-color: #ffffff; margin: 0 20px;\"></div>\n",
        "            <h3 style=\"color: #ffffff; margin-bottom: 10px; font-size: 20px;\">Confidence</h3>\n",
        "            <p style=\"font-size: 36px; font-weight: 800; color: #2196F3; margin: 0\">\n",
        "              {prediction[0][class_index]:.4%}\n",
        "            </p>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "      \"\"\",\n",
        "      unsafe_allow_html=True\n",
        "  )\n",
        "\n",
        "  # Prepare data for Plotly chart\n",
        "  probabilities = prediction[0]\n",
        "  sorted_indices = np.argsort(probabilities)\n",
        "  sorted_labels = [labels[i] for i in sorted_indices]\n",
        "  sorted_probabilities = probabilities[sorted_indices]\n",
        "\n",
        "  # Create a Plotly bar chart\n",
        "  fig = go.Figure(go.Bar(\n",
        "    x=sorted_probabilities,\n",
        "    y=sorted_labels,\n",
        "    orientation='h',\n",
        "    marker_color=['red' if label == result else 'blue' for label in sorted_labels]\n",
        "  ))\n",
        "\n",
        "  # Customize the chart layout\n",
        "  fig.update_layout(\n",
        "    title='Probabilities for each class',\n",
        "    xaxis_title='Probability',\n",
        "    yaxis_title='Class',\n",
        "    height=400,\n",
        "    width=600,\n",
        "    yaxis=dict(autorange=\"reversed\")\n",
        "  )\n",
        "\n",
        "  # Add value labels to the bars\n",
        "  for i, prob in enumerate(sorted_probabilities):\n",
        "    fig.add_annotation(\n",
        "        x=prob,\n",
        "        y=i,\n",
        "        text=f\"{prob:.4f}\",\n",
        "        showarrow=False,\n",
        "        xanchor='left',\n",
        "        xshift=5\n",
        "    )\n",
        "  # Display the Plotly chart\n",
        "  st.plotly_chart(fig)\n",
        "\n",
        "  saliency_map_path = f\"saliency_maps/{uploaded_file.name}\"\n",
        "\n",
        "\n",
        "\n",
        "  selected_gen_model = st.radio(\n",
        "    \"Select Generative Model for Explanation\",\n",
        "    (\"Gemini 1.5 Flash\", \"Llama 3.2-3b Preview\")\n",
        "  )\n",
        "\n",
        "  with st.spinner('Generating explanation...'):\n",
        "        if selected_gen_model == \"Gemini 1.5 Flash\":\n",
        "            explanation = generate_explanation(saliency_map_path, result, prediction[0][class_index], \"Gemini\")\n",
        "        else:\n",
        "            explanation = generate_explanation(saliency_map_path, result, prediction[0][class_index], \"Llama\")\n",
        "\n",
        "        st.write(\"## Explanation\")\n",
        "        st.write(explanation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoMs2Fo0dyHV",
        "outputId": "103c02cb-649a-4ce9-d7f2-61c1eabaeb57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thread = Thread(target=run_streamlit)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "n8yzoEvTd-hH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n",
        "\n",
        "print(\"Public URL:\",public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRD9rEMQetKh",
        "outputId": "3c3e3e67-eda1-45d9-d14d-24a748b7eaf6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://16f7-34-19-108-246.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tunnels = ngrok.get_tunnels()\n",
        "for tunnel in tunnels:\n",
        "  print(f\"Closing tunnel: {tunnel.public_url} -> {tunnel.config['addr']}\")\n",
        "  ngrok.disconnect(tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOkmwLWggGp9",
        "outputId": "4ae86498-1e62-4e20-caa8-c8bb136abc2e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:58:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-c42d8883-e3c4-411f-96ef-feb4d92b21cb acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:58:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-207ad146-37a6-45fc-8992-f8e942575457 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-13T01:58:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-7cf5564e-7fd5-4486-8959-d3db04ac2060 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing tunnel: https://9ed0-34-19-108-246.ngrok-free.app -> http://localhost:8501\n",
            "Closing tunnel: https://3c3d-34-19-108-246.ngrok-free.app -> http://localhost:8501\n",
            "Closing tunnel: https://8cb7-34-19-108-246.ngrok-free.app -> http://localhost:8501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "\n",
        "GOOGLE_API_KEY=AIzaSyCfOIlrlbAaw4hM17kMSnKqfP1py4B9FhA"
      ],
      "metadata": {
        "id": "Ubs6AIog6XIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697c5f96-a354-4349-e4e7-8e3d2a8a8f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPipgZsJ-t3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}